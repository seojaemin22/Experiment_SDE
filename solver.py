import jax
import flax.serialization as fs
from jax.experimental.jet import jet
import optax
from jax import numpy as jnp
import numpy as np
from flax import linen as nn
from flax.core import freeze
import tqdm
import wandb
import matplotlib.pyplot as plt
from model import *
from config import *
from utils import *
from functools import partial
import copy
from pathlib import Path

from flax.traverse_util import flatten_dict

class Solver():

    def __init__(self, model_config: Model_Config, solver_config: Solver_Config, problem_config: Problem_Config):
        self.model_config = model_config
        self.solver_config = solver_config
        self.problem_config = problem_config  
        
        self.model = get_model(self.model_config, self.problem_config.problem_name)
        self.grad_setting()
        self.problem_setting()
        self.grad_fn = self.loss_to_grad(self.get_loss())
        self.optimizer = self.create_opt()

        self.sol_T, self.sol_X, self.sol_U = self.get_analytic_sol()
        if self.solver_config.custom_eval:
            self.eval_data = self.get_eval_data()
        if self.solver_config.save_to_wandb:
            self.init_wandb()

    def grad_setting(self):
        def bind_apply(f):
            apply_fn = self.model.apply
            def wrapped(*args, **kwargs):
                return f(apply_fn, *args, **kwargs)
            return wrapped

        def maybe_ckpt(f):
            return jax.checkpoint(f) if self.solver_config.checkpointing else f

        self.calc_u      = maybe_ckpt(bind_apply(self._calc_u))
        self.calc_ut     = maybe_ckpt(bind_apply(self._calc_ut))
        self.calc_ux     = maybe_ckpt(bind_apply(self._calc_ux))
        self.calc_ut_ux  = maybe_ckpt(bind_apply(self._calc_ut_ux))
        self.calc_uxx    = maybe_ckpt(bind_apply(self._calc_uxx))

        if self.model_config.derivative == 'forward':
            self.calc_laplacian = maybe_ckpt(bind_apply(self._forward_laplacian))
        elif self.model_config.derivative == 'backward':
            self.calc_laplacian = maybe_ckpt(bind_apply(self._calc_laplacian))

    def problem_setting(self):
        problem_name = self.problem_config.problem_name
        bc_name = self.model_config.bc_name

        self.bc_fn = get_boundary_function(problem_name, bc_name)

        self.default_domain = getattr(self, f'{problem_name}_default_domain')
        self.get_X0 = getattr(self, f'{problem_name}_get_X0')
        self.get_exact_X0 = getattr(self, f'{problem_name}_get_exact_X0')

        self.analytic_X = getattr(self, f'{problem_name}_{bc_name}_analytic_X')
        self.analytic_u = getattr(self, f'{problem_name}_{bc_name}_analytic_u')
        
        self.pinns_residual = getattr(self, f'{problem_name}_pinns_residual')
        self.b = getattr(self, f'{problem_name}_b')
        self.sigma = getattr(self, f'{problem_name}_sigma')
        self.h = getattr(self, f'{problem_name}_h')
        self.b_heun = getattr(self, f'{problem_name}_b_heun')  # b + Correction of Forward Stratonovich SDE (- 1/2 sigma sigma_x)

    def c(self, weighted_lap):  # Correction of Backward Stratonovich SDE
        return 0.5 * weighted_lap
    
    def get_loss(self):
        loss_method = self.solver_config.loss_method
        if loss_method == 'fspinns': return self.fspinns_loss
        elif loss_method == 'fbsnn': return self.fbsnn_loss
        elif loss_method == 'fbsnnheun': return self.fbsnnheun_loss
        elif loss_method == 'nobiasfbsnn': return self.nobiasfbsnn_loss
        elif loss_method == 'shotgun': return self.shotgun_loss
        elif loss_method == 'nobiasshotgun': return self.nobiasshotgun_loss
        else: raise Exception("Loss Method '" + loss_method + "' Not Implemented")


    def create_opt(self):
        if self.solver_config.schedule == 'piecewise_constant':
            schedule = optax.piecewise_constant_schedule(
                init_value=self.solver_config.lr,
                boundaries_and_scales=self.solver_config.boundaries_and_scales
            )
        elif self.solver_config.schedule == 'cosine_decay':
            schedule = optax.cosine_decay_schedule(
                init_value=self.solver_config.lr,
                decay_steps=self.solver_config.iter
            )
        elif self.solver_config.schedule == 'cosine_onecycle':
            schedule = optax.cosine_onecycle_schedule(
                transition_steps=self.solver_config.iter,
                peak_value=self.solver_config.lr
            )
        else: # No schedule
            schedule = optax.constant_schedule(
                value=self.solver_config.lr
            )
            
        if self.solver_config.optim == 'adam':
            return optax.chain(
                optax.scale_by_adam(),
                optax.scale_by_schedule(schedule),
                optax.scale(-1.0)
            )
        else: # SGD
            return optax.chain(
                optax.scale_by_schedule(schedule),
                optax.scale(-1.0)
            )
        
    def init_model(self, key: Key):
        key, sub = key.split()
        t_pde = jax.random.uniform(sub, (self.solver_config.micro_batch, 1), minval=0, maxval=self.solver_config.T)

        key, x_pde = self.default_domain(key, t_pde)
        key, sub = key.split()

        return key, self.model.init(sub, t_pde, x_pde)

    def init_opt(self,params):
        return self.optimizer.init(params)
    
    def init_solver(self, key: Key):
        key, params = self.init_model(key)
        opt_state = self.init_opt(params)
        
        model_path = Path(self.solver_config.model_state)
        if model_path.exists():
            model_bytes = model_path.read_bytes()
            params = fs.from_bytes(params, model_bytes)
        
        opt_path = Path(self.solver_config.opt_state)
        if opt_path.exists():
            opt_bytes = opt_path.read_bytes()
            opt_state = fs.from_bytes(opt_state, opt_bytes)

        num_params = sum(x.size for x in jax.tree_util.tree_leaves(params))
        if self.solver_config.save_to_wandb:
            wandb.config['# Params'] =  num_params
        return key, params, opt_state

    # --------------------------------------------------
    # Problem Setting
    # --------------------------------------------------

    def HJB_default_domain(self, key: Key, t_pde):
        key, sub = key.split()
        x_pde = jnp.sqrt(2) * jax.random.normal(sub, (*t_pde.shape[:-1], self.model_config.d_in))
        return key, x_pde
    
    def HJB_get_X0(self, key, batch):
        key, sub = key.split()
        X0 = jax.random.normal(sub, (batch, self.model_config.d_in)) * self.solver_config.X0_std
        return key, X0
    
    def HJB_get_exact_X0(self):
        return jnp.zeros(self.model_config.d_in)
    
    
    def HJB_default_analytic_X(self, T, W):
        return jnp.broadcast_to(self.HJB_get_exact_X0()[jnp.newaxis, jnp.newaxis, :], (T.shape[0], 1, self.model_config.d_in)) + jnp.sqrt(2.0)*W

    def HJB_default_analytic_u(self, t, x):
        w = jnp.sqrt(self.solver_config.T-t) * jax.random.normal(jax.random.key(10), (100000, self.model_config.d_in))
        return -jnp.log(jnp.mean(jnp.exp(-self.bc_fn(x + jnp.sqrt(2)*w)), axis=0))


    def HJB_b(self, t, x):
        return jnp.zeros_like(x)

    def HJB_sigma(self, t, x):
        return jnp.sqrt(2) * jnp.broadcast_to(jnp.eye(x.shape[-1]), (*x.shape[:-1], x.shape[-1], x.shape[-1]))
    
    def HJB_h(self, t, x, y, z):
        return jnp.sum(z**2, axis=-1)

    def HJB_b_heun(self, t, x):
        return jnp.zeros_like(x)
    
    def HJB_pinns_residual(self, params, t, x):
        _, ut, ux = self.calc_ut_ux(params, t, x)
        _, lap = self.calc_laplacian(params, t, x)
        loss = jnp.mean((ut[..., 0] + lap - jnp.sum(ux**2, axis=-1))**2)
        return loss
    
    # --------------------

    def BSB_default_domain(self, key: Key, t_pde):
        key, sub = key.split()
        x_pde = 0.75 + jax.random.normal(sub, (*t_pde.shape[:-1], self.model_config.d_in))
        return key, x_pde

    def BSB_get_X0(self, key, batch):
        key, sub = key.split()
        X0_list = []
        for i in range(self.model_config.d_in):
            X0_list.append(jnp.ones((batch, 1))/2 if i%2 == 1 else jnp.ones((batch, 1))) 
        X0 = jnp.concatenate(X0_list, axis=-1) + jax.random.normal(sub, (batch, self.model_config.d_in)) * self.solver_config.X0_std
        return key, X0

    def BSB_get_exact_X0(self):
        return jnp.concatenate([(jnp.ones(1) if i%2 == 0 else jnp.ones(1)/2) for i in range(self.model_config.d_in)])
    

    def BSB_default_analytic_X(self, T, W):
        return jnp.broadcast_to(self.BSB_get_exact_X0()[jnp.newaxis, jnp.newaxis, :], (T.shape[0], 1, self.model_config.d_in)) * jnp.exp(0.4*W - 0.5*0.4**2*T)

    def BSB_default_analytic_u(self, t, x):
        return jnp.exp((0.05 + 0.4**2)*(self.solver_config.T - t)) * self.bc_fn(x)
    

    def BSB_b(self, t, x):
        return jnp.zeros_like(x)
    
    def BSB_sigma(self, t, x):
        return 0.4 * jax.vmap(jnp.diag, in_axes=0)(x)
    
    def BSB_h(self, t, x, y, z):
        return 0.05 * (y - jnp.matmul(z, x[..., jnp.newaxis])[..., 0])
    
    def BSB_b_heun(self, t, x):
        return -0.5 * 0.4**2 * x
    
    def BSB_pinns_residual(self, params, t, x):
        u, ut, ux = self.calc_ut_ux(params, t, x)
        _, weighted_lap = self.calc_laplacian(params, t, x, weight=self.BSB_sigma(t, x))
        loss = jnp.mean((ut[..., 0] + 0.5*weighted_lap - 0.05*(u - jnp.matmul(ux, x[..., jnp.newaxis])[..., 0]))**2)
        return loss

    # --------------------------------------------------
    # Calculation Methods
    # --------------------------------------------------
    
    def calc_bcx(self, x): 
        jax_x = jax.jacrev(lambda x: self.bc_fn(x), argnums=0)
        return jax.vmap(jax_x, in_axes=0)(x)

    # --------------------------------------------------

    def _calc_u(self, apply_fn, params, t, x):
        return apply_fn(params, t, x)
    
    def _calc_ut(self, apply_fn, params, t, x):
        def u_ut(t, x):
            model_fn = lambda tt: self._calc_u(apply_fn, params, tt, x)
            u, du_dt = jax.vjp(model_fn, t)
            ut = jax.vmap(du_dt, in_axes=0)(jnp.eye(len(u)))[0]
            return u, ut
        return jax.vmap(u_ut, in_axes=(0, 0))(t, x)

    def _calc_ux(self, apply_fn, params, t, x):
        def u_ux(t, x):
            model_fn = lambda xx: self._calc_u(apply_fn, params, t, xx)
            u, du_dx = jax.vjp(model_fn, x)
            ux = jax.vmap(du_dx, in_axes=0)(jnp.eye(len(u)))[0]
            return u, ux
        return jax.vmap(u_ux, in_axes=(0, 0))(t, x)

    def _calc_ut_ux(self, apply_fn, params, t, x):
        model_fn = lambda ttxx: self._calc_u(apply_fn, params, ttxx[..., :1], ttxx[..., 1:])
        def u_ut_ux(tx):
            u, du_dx_dt = jax.vjp(model_fn, tx)
            ux_ut = jax.vmap(du_dx_dt, in_axes=0)(jnp.eye(len(u)))[0]
            return u, ux_ut[..., :1], ux_ut[..., 1:]
        return jax.vmap(u_ut_ux, in_axes=0)(jnp.concatenate((t, x), axis=-1))

    def _calc_uxx(self, apply_fn, params, t, x):
        def u_ux_uxx(t, x):
            model_fn = lambda xx: self._calc_u(apply_fn, params, t, xx)
            def ux_u(x):
                u, du_dx = jax.vjp(model_fn, x)
                ux = jax.vmap(du_dx, in_axes=0)(jnp.eye(len(u)))[0]
                return ux, u
            du_dxx = lambda s: jax.jvp(ux_u, (x,), (s,), has_aux=True)
            ux, uxx, u = jax.vmap(du_dxx, in_axes=1, out_axes=(None, 1, None))(jnp.eye(len(x)))
            return u, ux, uxx
        return jax.vmap(u_ux_uxx, in_axes=(0, 0))(t, x)

    def _calc_laplacian(self, apply_fn, params, x, t=None, weight=None):
        weight = jnp.broadcast_to(
            weight if weight is not None else jnp.eye(self.model_config.d_in),
            shape=(x.shape[0], self.model_config.d_in, self.model_config.d_in)
        )
        H = jnp.einsum('bij,bkj->bik', weight, weight)  # H = weight weight^T
        u, _, uxx = self._calc_uxx(apply_fn, params, t, x)
        return u, jnp.einsum('bmij,bij->bm', uxx, H)  # Tr(H uxx)
    
    def _forward_laplacian(self, apply_fn, params, t, x, weight=None):
        return self.model.forward_laplacian(params, t, x, weight=weight)

    # --------------------------------------------------

    def analytic_ut(self, t, x):
        def u_ut(t, x):
            model_fn = lambda tt: self.analytic_u(tt, x)
            u, du_dt = jax.vjp(model_fn, t)
            ut = jax.vmap(du_dt, in_axes=0)(jnp.eye(len(u)))[0]
            return u, ut
        return jax.vmap(u_ut, in_axes=(0, 0))(t, x)
    
    def analytic_ux(self, t, x):
        def u_ux(t, x):
            model_fn = lambda xx: self.analytic_u(t, xx)
            u, vjp_fun = jax.vjp(model_fn, x)
            ux = jax.vmap(vjp_fun, in_axes=0)(jnp.eye(len(u)))[0]
            return u, ux
        return jax.vmap(u_ux, in_axes=(0, 0))(t, x)
    
    def analytic_ut_ux(self, t, x):
        model_fn = lambda ttxx: self.analytic_u(ttxx[:1], ttxx[1:])
        def u_ut_ux(tx):
            u, du_dx_dt = jax.vjp(model_fn, tx)
            ux_ut = jax.vmap(du_dx_dt, in_axes=0)(jnp.eye(len(u)))[0]
            return u, ux_ut[:1], ux_ut[1:]
        return jax.vmap(u_ut_ux, in_axes=0)(jnp.concatenate((t, x), axis=-1))
    
    def analytic_uxx(self, t, x):
        def u_ux_uxx(t, x):
            model_fn = lambda xx: self.analytic_u(t, xx)
            def ux_x(x):
                u, du_dx = jax.vjp(model_fn, x)
                ux = jax.vmap(du_dx, in_axes=0)(jnp.eye(len(u)))[0]
                return ux, u
            du_dxx = lambda s: jax.jvp(ux_x, (x,), (s,), has_aux=True)
            ux, uxx, u = jax.vmap(du_dxx, in_axes=1, out_axes=(None, 1 ,None))(jnp.eye(len(x)))
            return u, ux, uxx
        return jax.vmap(u_ux_uxx, in_axes=(0, 0))(t, x)
    
    # --------------------------------------------------
    # Loss Methods  [ time-coupled model ]
    # --------------------------------------------------

    def loss_to_grad(self, loss_fn):
        def _loss_and_grad(key, params, causal_step):
            (total, (losses, key, params)), grad = jax.value_and_grad(lambda K, P: loss_fn(K, P, causal_step), argnums=1, has_aux=True)(key, params)
            return (total, (losses, key, params)), grad

        def grad_fn(key, params, causal_step):
            n_chunks = (self.solver_config.batch + self.solver_config.micro_batch - 1) // self.solver_config.micro_batch

            def chunk_loop(carry, _):
                key, params, losses_acc, grad_acc = carry
                (total, (losses, key, params)), grad = _loss_and_grad(key, params, causal_step)
                losses_vec = jnp.asarray(losses)
                losses_acc = losses_acc + losses_vec
                grad_acc = jax.tree_util.tree_map(lambda a, b: a+b, grad_acc, grad)
                return (key, params, losses_acc, grad_acc), None
            
            losses_0 = jnp.zeros((len(loss_fn(key, params, causal_step)[1][0]),), dtype=jnp.result_type(0.0))
            grad_0 = jax.tree_util.tree_map(jnp.zeros_like, params)
            (key, params, losses, grad), _ = jax.lax.scan(chunk_loop, (key, params, losses_0, grad_0), None, length=n_chunks)
            return key, params, losses, grad
        
        return grad_fn
        
    # --------------------------------------------------

    def make_time_domain(self, key: Key, batch: int):
        if self.solver_config.use_delta:
            dt = self.solver_config.T / (self.solver_config.traj_len - 1)
            key, sub = key.split()
            t1 = jax.random.uniform(sub, (batch, 1), minval=0, maxval=dt)
            diffT = jnp.concatenate([jnp.zeros((batch, 1)),
                                     t1,
                                     jnp.full((batch, self.solver_config.traj_len-2), fill_value=dt), 
                                     jnp.full_like(t1, fill_value=dt) - t1], axis=-1)
            T = jnp.cumsum(diffT, axis=-1)[:, :, None]
        else:
            dt = self.solver_config.T / self.solver_config.traj_len
            T = jnp.broadcast_to(jnp.linspace(0, self.solver_config.T, self.solver_config.traj_len+1)[None, :, None], (batch, self.solver_config.traj_len+1, 1))

        dT = T[:, 1:, :] - T[:, :-1, :]
        return key, T, dT
    
    def make_full_domain_euler(self, key: Key, batch: int):
        key, T, dT = self.make_time_domain(key, batch)
        
        key, sub = key.split()
        dW = jnp.sqrt(dT) * jax.random.normal(sub, (batch, self.solver_config.traj_len, self.model_config.d_in))

        X = jnp.zeros((batch, self.solver_config.traj_len+1, self.model_config.d_in))
        key, X0 = self.get_X0(key, batch)
        X = X.at[:, 0, :].set(X0)

        def loop(i, X):
            X = X.at[:, i, :].set(X[:, i-1, :] + self.b(T[:, i-1, :], X[:, i-1, :])*dT[:, i-1, :]
                                + jnp.matmul(self.sigma(T[:, i-1, :], X[:, i-1, :]), dW[:, i-1, :, jnp.newaxis])[..., 0])
            return X
        X = jax.lax.fori_loop(1, self.solver_config.traj_len+1, loop, X)
        return key, T, dT, dW, X
    
    def make_full_domain_heun(self, key: Key, batch: int):
        key, T, dT = self.make_time_domain(key, batch)
        
        key, sub = key.split()
        dW = jnp.sqrt(dT) * jax.random.normal(sub, (batch, self.solver_config.traj_len, self.model_config.d_in))

        X = jnp.zeros((batch, self.solver_config.traj_len+1, self.model_config.d_in))
        key, X0 = self.get_X0(key, batch)
        X = X.at[:, 0, :].set(X0)

        X_star = jnp.zeros((batch, self.solver_config.traj_len, self.model_config.d_in))

        def loop(i, inputs):
            X, X_star = inputs
            
            dX1 = self.b_heun(T[:, i-1, :], X[:, i-1, :])*dT[:, i-1, :] + jnp.matmul(self.sigma(T[:, i-1, :], X[:, i-1, :]), dW[:, i-1, :, jnp.newaxis])[..., 0]
            X_star = X_star.at[:, i-1, :].set(X[:, i-1, :] + dX1) 
            X = X.at[:, i, :].set(X[:, i-1, :] + 0.5 * (dX1 + (self.b_heun(T[:, i, :], X_star[:, i-1, :])*dT[:, i-1, :] 
                                                               + jnp.matmul(self.sigma(T[:, i, :], X_star[:, i-1, :]), dW[:, i-1, :, jnp.newaxis])[...,0])))
            return X, X_star
        X, X_star = jax.lax.fori_loop(1, self.solver_config.traj_len+1, loop, (X, X_star))
        return key, T, dT, dW, X, X_star
    
    # --------------------------------------------------

    def compute_balanced_loss(self, pde_losses, epsilon):
        if self.solver_config.causal_training:
            weight = jnp.concatenate([jax.lax.stop_gradient(jnp.exp(-epsilon * jnp.cumsum(pde_losses[:-1][::-1])[::-1])), jnp.ones(1)])
            weight = jnp.where(weight > 0.99, weight, 0.0)
            jax.debug.print("{}, ..., {}, {}", weight[0], weight[-2], weight[-1])
            return jnp.sum(weight * pde_losses)                                                                                                             
        else:
            return jnp.sum(pde_losses)

    # --------------------------------------------------
    # --------------------------------------------------

    def fspinns_loss(self, key, params, epsilon):
        batch = self.solver_config.micro_batch
        
        key, T, dT, dW, X = self.make_full_domain_euler(key, batch)
        
        pde_loss = self.solver_config.pde_scale * self.pinns_residual(params, T.reshape(-1, 1), X.reshape(-1, self.model_config.d_in))
        if self.model_config.use_hard_constraint:
            return pde_loss, ((pde_loss,), key)
        else:
            u, ux = self.calc_ux(params, T[:, -1, :], X[:, -1, :])
            bc_loss = self.model_config.bc_scale * (jnp.mean((u - self.bc_fn(X[:, -1, :]))**2) + jnp.mean((ux - self.calc_bcx(X[:, -1, :]))**2))
            return pde_loss + bc_loss, ((pde_loss, bc_loss), key, params)
    
    # --------------------------------------------------

    def fbsnn_loss(self, key, params, epsilon):
        batch = self.solver_config.micro_batch
        
        key, T, dT = self.make_time_domain(key, batch)
        key, x_start = self.get_X0(key, batch)
        u_start, ux_start = self.calc_ux(params, T[:, 0, :], x_start)
        traj_loss = jnp.zeros(self.solver_config.traj_len)

        def traj_calc(i, inputs):
            key, x, u, ux, traj_loss = inputs

            t = T[:, i, :]
            dt = dT[:, i, :]
            sigma = self.sigma(t, x)

            key, sub = key.split()
            dw = jnp.sqrt(dt) * jax.random.normal(sub, (batch, self.model_config.d_in))

            t_new = T[:, i+1, :]  # t + dt
            x_new = x + self.b(t, x)*dt + jnp.matmul(sigma, dw[..., jnp.newaxis])[..., 0]
            u_new = u + self.h(t, x, u, ux)*dt + jnp.matmul(jnp.matmul(ux, sigma), dw[..., jnp.newaxis])[..., 0]

            u_calc, ux_calc = self.calc_ux(params, t_new, x_new)
            traj_loss = traj_loss.at[i].set(jnp.mean(self.solver_config.pde_scale * (u_new - u_calc)**2))

            return key, x_new, u_calc, ux_calc, traj_loss

        key, x_end, u_end, ux_end, traj_loss = jax.lax.fori_loop(0, self.solver_config.traj_len, traj_calc, (key, x_start, u_start, ux_start, traj_loss))

        pde_loss = self.compute_balanced_loss(traj_loss, epsilon=epsilon)
        if self.model_config.use_hard_constraint:
            return pde_loss, ((pde_loss,), key, params)
        else:
            bc_loss = self.model_config.bc_scale * (jnp.mean((u_end - self.bc_fn(x_end))**2) + jnp.mean((ux_end - self.calc_bcx(x_end))**2))
            return pde_loss + bc_loss, ((pde_loss, bc_loss), key, params)

    # --------------------------------------------------   

    def fbsnnheun_loss(self, key, params, epsilon):
        batch = self.solver_config.micro_batch

        key, T, dT = self.make_time_domain(key, batch)
        key, x_start = self.get_X0(key, batch)
        u_start, ux_start = self.calc_ux(params, T[:, 0, :], x_start)
        traj_loss = jnp.zeros(self.solver_config.traj_len)

        def traj_calc(i, inputs):
            key, x, u, ux, traj_loss = inputs

            t = T[:, i, :]
            dt = dT[:, i, :]
            sigma = self.sigma(t, x)
            weighted_lap = self.calc_laplacian(params, x, t, weight=sigma)[1]

            key, sub = key.split()
            dw = jnp.sqrt(dt) * jax.random.normal(sub, (batch, self.model_config.d_in))
            
            dx_star = self.b_heun(t, x)*dt + jnp.matmul(sigma, dw[..., jnp.newaxis])[..., 0]
            x_star = x + dx_star
            du_star = (self.h(t, x, u, ux) - self.c(weighted_lap))*dt + jnp.matmul(jnp.matmul(ux, sigma), dw[..., jnp.newaxis])[..., 0]
            u_star = u + du_star

            t_new = T[:, i+1, :]  # t + dt
            _, ux_star = self.calc_ux(params, t_new, x_star)
            sigma_star = self.sigma(t_new, x_star)
            weighted_lap_star = self.calc_laplacian(params, x_star, t_new, weight=sigma_star)[1]

            x_new = x + 0.5*dx_star + 0.5*(self.b_heun(t_new, x_star)*dt + jnp.matmul(sigma_star, dw[..., jnp.newaxis])[..., 0])
            u_new = u + 0.5*du_star + 0.5*((self.h(t_new, x_star, u_star, ux_star) - self.c(weighted_lap_star))*dt + jnp.matmul(jnp.matmul(ux_star, sigma_star), dw[..., jnp.newaxis])[..., 0])

            u_calc, ux_calc = self.calc_ux(params, t_new, x_new)
            traj_loss = traj_loss.at[i].set(jnp.mean((u_new - u_calc)**2))

            return key, x_new, u_calc, ux_calc, traj_loss

        key, x_end, u_end, ux_end, traj_loss = jax.lax.fori_loop(0, self.solver_config.traj_len, traj_calc, (key, x_start, u_start, ux_start, traj_loss))
        pde_loss = self.solver_config.pde_scale * self.compute_balanced_loss(traj_loss, epsilon=epsilon)
        if self.model_config.use_hard_constraint:
            return pde_loss, ((pde_loss,), key, params)
        else:
            bc_loss = self.model_config.bc_scale * (jnp.mean((u_end - self.bc_fn(x_end))**2) + jnp.mean((ux_end - self.calc_bcx(x_end))**2))
            return pde_loss + bc_loss, ((pde_loss, bc_loss), key, params)
    
    # --------------------------------------------------

    def nobiasfbsnn_loss(self, key, params):
        batch = self.solver_config.micro_batch
        
        key, T, dT = self.make_time_domain(key, batch)
        key, x_start = self.get_X0(key, batch)
        u_start, ux_start = self.calc_ux(params, T[:, 0, :], x_start)
        traj_loss = jnp.zeros(self.solver_config.traj_len)

        def traj_calc(i, inputs):
            key, x, u, ux, traj_loss = inputs

            t = T[:, i, :]
            dt = dT[:, i, :]
            sigma = self.sigma(t, x)

            t_new = T[:, i+1, :]  # t + dt

            key, sub = key.split()
            dw1 = jnp.sqrt(dt) * jax.random.normal(sub, (batch, self.model_config.d_in))
            x_new1 = x + self.b(t, x)*dt + jnp.matmul(sigma, dw1[..., jnp.newaxis])[..., 0]
            u_new1 = u + self.h(t, x, u, ux)*dt + jnp.matmul(jnp.matmul(ux, sigma), dw1[..., jnp.newaxis])[..., 0]

            key, sub = key.split()
            dw2 = jnp.sqrt(dt) * jax.random.normal(sub, (batch, self.model_config.d_in))
            x_new2 = x + self.b(t, x)*dt + jnp.matmul(sigma, dw2[..., jnp.newaxis])[..., 0]
            u_new2 = u + self.h(t, x, u, ux)*dt + jnp.matmul(jnp.matmul(ux, sigma), dw2[..., jnp.newaxis])[..., 0]

            u_calc1, ux_calc1 = self.calc_ux(params, t_new, x_new1)
            u_calc2 = self.calc_u(params, t_new, x_new2)
            traj_loss = traj_loss.at[i].set(jnp.mean((u_new1 - u_calc1)*(u_new2 - u_calc2)))

            return key, x_new1, u_calc1, ux_calc1, traj_loss

        key, x_end, u_end, ux_end, traj_loss = jax.lax.fori_loop(0, self.solver_config.traj_len, traj_calc, (key, x_start, u_start, ux_start, traj_loss))
        pde_loss = self.solver_config.pde_scale * jnp.sum(traj_loss)
        if self.model_config.use_hard_constraint:
            return pde_loss, ((pde_loss,), key, params)
        else:
            bc_loss = self.model_config.bc_scale * (jnp.mean((u_end - self.bc_fn(x_end))**2) + jnp.mean((ux_end - self.calc_bcx(x_end))**2))
            return pde_loss + bc_loss, ((pde_loss, bc_loss), key, params)

    # --------------------------------------------------

    def shotgun_loss(self, key, params):
        batch = self.solver_config.micro_batch
        
        key, T, dT = self.make_time_domain(key, batch)
        key, x_start = self.get_X0(key, batch)
        u_start, ux_start = self.calc_ux(params, T[:, 0, :], x_start)
        step_loss = jnp.zeros(self.solver_config.traj_len)
        
        Delta_t = self.solver_config.shotgun_Delta_t
        local_batch = self.solver_config.shotgun_local_batch
        d_in = self.model_config.d_in
        
        def traj_calc(i, inputs):
            key, x, u, ux, step_loss = inputs

            t = T[:, i, :]
            dt = dT[:, i, :]
            sigma = self.sigma(t, x)
            
            t_local = jnp.broadcast_to(t[:, jnp.newaxis, :] + Delta_t, (batch, local_batch, 1))
            x_local = jnp.broadcast_to((x + self.b(t, x)*Delta_t)[:, jnp.newaxis, :], (batch, local_batch, d_in))
            u_local = jnp.broadcast_to(u[:, jnp.newaxis, 0], (batch, local_batch))
            h_local = jnp.broadcast_to(self.h(t, x, u, ux)[:, jnp.newaxis, 0], (batch, local_batch))

            key, sub = key.split()
            eta = jnp.sqrt(Delta_t) * jax.random.normal(sub, (batch, local_batch, d_in))

            diff = jnp.einsum('bij,bkj->bki', sigma, eta)
            x_plus = x_local + diff
            x_minus = x_local - diff
            
            u_plus = self.calc_u(params, t_local.reshape(-1, 1), x_plus.reshape(-1, d_in))
            u_minus = self.calc_u(params, t_local.reshape(-1, 1), x_minus.reshape(-1, d_in))
            step_loss = step_loss.at[i].set(
                jnp.mean(jnp.mean((u_plus.reshape(batch, local_batch) + u_minus.reshape(batch, local_batch) - 2*u_local)/(2*Delta_t) - h_local, axis=-1)**2)
            )

            key, sub = key.split()
            dw = jnp.sqrt(dt) * jax.random.normal(sub, (batch, self.model_config.d_in))

            t_new = T[:, i+1, :]  # t + dt 
            x_new = x + self.b(t, x)*dt + jnp.matmul(sigma, dw[..., jnp.newaxis])[..., 0]
            # u_new = u + self.h(t, x, u, ux)*dt + jnp.matmul(jnp.matmul(ux, sigma), dw[..., jnp.newaxis])[..., 0]

            u_calc, ux_calc = self.calc_ux(params, t_new, x_new)
                
            return key, x_new, u_calc, ux_calc, step_loss
        
        key, x_end, u_end, ux_end, step_loss = jax.lax.fori_loop(0, self.solver_config.traj_len, traj_calc, (key, x_start, u_start, ux_start, step_loss))
        pde_loss = self.solver_config.pde_scale * jnp.mean(step_loss)
        if self.model_config.use_hard_constraint:
            return pde_loss, ((pde_loss,), key, params)
        else:
            bc_loss = self.model_config.bc_scale * (jnp.mean((u_end - self.bc_fn(x_end))**2) + jnp.mean((ux_end - self.calc_bcx(x_end))**2))
            return pde_loss + bc_loss, ((pde_loss, bc_loss), key, params)
    
    # --------------------------------------------------
    
    def nobiasshotgun_loss(self, key, params):
        batch = self.solver_config.micro_batch
        
        key, T, dT = self.make_time_domain(key, batch)
        key, x_start = self.get_X0(key, batch)
        u_start, ux_start = self.calc_ux(params, T[:, 0, :], x_start)
        step_loss = jnp.zeros(self.solver_config.traj_len)
        
        Delta_t = self.solver_config.shotgun_Delta_t
        local_batch = self.solver_config.shotgun_local_batch
        d_in = self.model_config.d_in
        
        def traj_calc(i, inputs):
            key, x, u, ux, step_loss = inputs

            t = T[:, i, :]
            dt = dT[:, i, :]
            sigma = self.sigma(t, x)
            
            t_local = jnp.broadcast_to(t[:, jnp.newaxis, :] + Delta_t, (batch, local_batch, 1))
            x_local = jnp.broadcast_to((x + self.b(t, x)*Delta_t)[:, jnp.newaxis, :], (batch, local_batch, d_in))
            u_local = jnp.broadcast_to(u[:, jnp.newaxis, 0], (batch, local_batch))
            h_local = jnp.broadcast_to(self.h(t, x, u, ux)[:, jnp.newaxis, 0], (batch, local_batch))

            # key, sub = key.split()
            # eta = jnp.sqrt(Delta_t) * jax.random.normal(sub, (batch, local_batch, d_in))

            key, sub = key.split()
            G = jax.random.normal(sub, shape=(batch, d_in, local_batch))
            Q, R = jnp.linalg.qr(G, mode='reduced')

            diagR = jnp.diagonal(R, axis1=-2, axis2=-1)
            signs = jnp.sign(diagR)
            signs = jnp.where(signs == 0, jnp.array(1.), signs)
            Q = Q * signs[:, jnp.newaxis, :]

            key, sub = key.split()
            radii = jnp.sqrt(Delta_t)[jnp.newaxis, jnp.newaxis] * jnp.sqrt(jax.random.chisquare(sub, df=d_in, shape=(batch, local_batch)))

            eta = jnp.transpose(Q * radii[:, jnp.newaxis, :], (0, 2, 1))

            # ------------------------------
            
            diff = jnp.einsum('bij,bkj->bki', sigma, eta)
            x_plus = x_local + diff
            x_minus = x_local - diff
            
            u_plus = self.calc_u(params, t_local.reshape(-1, 1), x_plus.reshape(-1, d_in))
            u_minus = self.calc_u(params, t_local.reshape(-1, 1), x_minus.reshape(-1, d_in))
            step_loss = step_loss.at[i].set(
                jnp.mean(jnp.mean((u_plus.reshape(batch, local_batch) + u_minus.reshape(batch, local_batch) - 2*u_local)/(2*Delta_t) - h_local, axis=-1)**2)
            )
            # step_loss = step_loss.at[i].set(
            #     jnp.mean(jnp.mean(((u_plus.reshape(batch, local_batch)[:, 0::2] + u_minus.reshape(batch, local_batch)[:, 0::2] - 2*u_local[:, 0::2])/(2*Delta_t) - h_local[:, ::2])*
            #                       ((u_plus.reshape(batch, local_batch)[:, 1::2] + u_minus.reshape(batch, local_batch)[:, 1::2] - 2*u_local[:, 1::2])/(2*Delta_t) - h_local[:, ::2]), axis=-1)**2)
            # )

            key, sub = key.split()
            dw = jnp.sqrt(dt) * jax.random.normal(sub, (batch, self.model_config.d_in))

            t_new = T[:, i+1, :]  # t + dt 
            x_new = x + self.b(t, x)*dt + jnp.matmul(sigma, dw[..., jnp.newaxis])[..., 0]
            # u_new = u + self.h(t, x, u, ux)*dt + jnp.matmul(jnp.matmul(ux, sigma), dw[..., jnp.newaxis])[..., 0]

            u_calc, ux_calc = self.calc_ux(params, t_new, x_new)
                
            return key, x_new, u_calc, ux_calc, step_loss
        
        key, x_end, u_end, ux_end, step_loss = jax.lax.fori_loop(0, self.solver_config.traj_len, traj_calc, (key, x_start, u_start, ux_start, step_loss))
        pde_loss = self.solver_config.pde_scale * jnp.mean(step_loss)
        if self.model_config.use_hard_constraint:
            return pde_loss, ((pde_loss,), key, params)
        else:
            bc_loss = self.model_config.bc_scale * (jnp.mean((u_end - self.bc_fn(x_end))**2) + jnp.mean((ux_end - self.calc_bcx(x_end))**2))
            return pde_loss + bc_loss, ((pde_loss, bc_loss), key, params)

    # --------------------------------------------------
    # Plot Methods
    # --------------------------------------------------

    def init_wandb(self):
        print("Initializing wandb")
        wandb.init(project=self.solver_config.project_name,
                   name=self.solver_config.run_name,
                   config={
                       'solver': vars(self.solver_config),
                       'model': vars(self.model_config),
                       'problem': vars(self.problem_config)
                   })

    def close(self):
        wandb.finish()


    def get_analytic_sol(self):
        num_traj = 1000
        test_dt = self.solver_config.T / self.solver_config.test_traj_len

        T = jnp.broadcast_to(jnp.linspace(0, self.solver_config.T, self.solver_config.test_traj_len+1)[None, :, None], (num_traj, self.solver_config.test_traj_len+1, 1))
        dW = jnp.sqrt(test_dt) * jnp.concatenate((jnp.zeros((num_traj, 1, self.model_config.d_in)),                                    
                                                  jax.random.normal(jax.random.key(1), (num_traj, self.solver_config.test_traj_len, self.model_config.d_in))), axis=1)
        W = jnp.cumsum(dW, axis=1)

        X = self.analytic_X(T, W)
        U = jax.lax.scan(lambda _, tx1: (None, jax.lax.scan(lambda _, tx2: (None, self.analytic_u(tx2[0], tx2[1])), None, (tx1[0], tx1[1]))[1]), None, (T, X))[1]
        return T, X, U
    

    def plot_pred(self, params, i):
        time = self.sol_T[:, :, 0].T
        pred = self.calc_u(params, self.sol_T, self.sol_X)[:, :, 0].T
        true = self.sol_U[:, :, 0].T
        L1 = jnp.mean(jnp.abs(pred - true) / jnp.abs(true), axis=1)
        L2 = jnp.sqrt(jnp.mean(((pred - true) ** 2) / (true ** 2), axis=1))

        fig_pred = plt.figure(figsize=(5, 3))
        plt.plot(time[:, :4], pred[:, :4], "r", linewidth=1)
        plt.plot(time[:, :4], true[:, :4], ":b", linewidth=1)
        plt.title('Prediction') 
        plt.close(fig_pred)
        wandb.log({'Prediction': wandb.Image(fig_pred)}, step=i)

        fig_L1 = plt.figure(figsize=(5, 3))
        plt.plot(time[:, 0], L1, "b", linewidth=1)
        plt.title('L1 Error')
        plt.yscale('log')
        plt.close(fig_L1)
        wandb.log({'L1 Error': wandb.Image(fig_L1)}, step=i)

        fig_L2 = plt.figure(figsize=(5, 3))
        plt.plot(time[:, 0], L2, "b", linewidth=1)
        plt.title('L2 Error')
        plt.yscale('log')
        plt.close(fig_L2)
        wandb.log({'L2 Error': wandb.Image(fig_L2)}, step=i)

    def calc_RL(self, params):
        pred = self.calc_u(params, self.sol_T, self.sol_X)
        true = self.sol_U

        RL1 = jnp.mean(jnp.abs(pred - true) / jnp.abs(true))
        RL2 = jnp.sqrt(jnp.mean(((pred - true) ** 2) / (true ** 2)))
        return RL1, RL2
    
    def calc_RL_T0(self, params):
        pred = self.calc_u(params, jnp.zeros(1), self.get_exact_X0())
        true = self.analytic_u(jnp.zeros(1), self.get_exact_X0())

        RL_T0 = jnp.mean(jnp.abs(pred - true) / jnp.abs(true))
        return pred, RL_T0
    
    @partial(jax.jit, static_argnums=0)
    def jit_calc_RL(self,params):
        return self.calc_RL(params)

    @partial(jax.jit, static_argnums=0)
    def jit_calc_RL_T0(self,params):
        return self.calc_RL_T0(params)


    def get_eval_data(self):
        pass

    def plot_eval(self, params):
        pass

    def calc_eval(self, params):
        pass

    @partial(jax.jit, static_argnums=0)
    def jit_calc_eval(self,params):
        return self.calc_eval(params)

    # --------------------------------------------------
    # Optimization Methods
    # --------------------------------------------------
    
    @partial(jax.jit, static_argnums=0)
    def optimize(self, key, params, opt_state, epsilon):
        key, params, losses, grad = self.grad_fn(key, params, epsilon)
        loss = jnp.sum(jnp.asarray(losses))
        updates, opt_state = self.optimizer.update(grad, opt_state)
        params = optax.apply_updates(params, updates)

        return key, loss, losses, params, opt_state



# --------------------------------------------------
# Control Class
# --------------------------------------------------

class Controller():

    def __init__(self, solver: Solver, seed=20226074):
        self.solver = solver
        self.key = Key(jax.random.PRNGKey(seed))
        self.key, self.params, self.opt_state = self.solver.init_solver(self.key)
        self.S = self.solver.solver_config.iter // len(self.solver.solver_config.epsilons)
        self.causal_step = 0

    def step(self, i):
        if self.solver.solver_config.causal_training:
            if i > 0 and i%(self.S) == 0:
                self.causal_step = self.causal_step + 1
            epsilon = jnp.asarray(self.solver.solver_config.epsilons[self.causal_step], dtype=jnp.float32)
            self.key, loss, losses, self.params, self.opt_state = self.solver.optimize(self.key, self.params, self.opt_state, epsilon=epsilon)
        else:
            self.key, loss, losses, self.params, self.opt_state = self.solver.optimize(self.key, self.params, self.opt_state, epsilon=0)

        if self.solver.solver_config.save_to_wandb:
            wandb.log({'loss': loss, **{'loss'+str(k+1): v for k, v in dict(enumerate(losses)).items()}}, step=i)

            pred_T0, RL_T0 = self.solver.jit_calc_RL_T0(self.params)
            wandb.log({'pred_T0': pred_T0, "RL_T0": RL_T0}, step=i)

            if self.solver.solver_config.custom_eval:
                custom_eval = self.solver.jit_calc_eval(self.params)
                wandb.log({"eval": custom_eval}, step=i)
            if i%(self.solver.solver_config.iter//self.solver.solver_config.num_figures) == 0:
                RL1, RL2 = self.solver.jit_calc_RL(self.params)
                wandb.log({"RL1": RL1, "RL2": RL2}, step=i)   
                self.solver.plot_pred(self.params, i)
                if self.solver.solver_config.custom_eval:
                    self.solver.plot_eval(self.params, i)

    def solve(self):
        for i in tqdm.tqdm(range(self.solver.solver_config.iter)):
            self.step(i)

        if self.solver.solver_config.save_to_wandb:
            self.solver.plot_pred(self.params, self.solver.solver_config.iter)
            if self.solver.solver_config.custom_eval:
                self.solver.plot_eval(self.params, self.solver.solver_config.iter)

        path = Path('./checkpoints/')
        path.mkdir(exist_ok=True)
        if self.solver.solver_config.save_model:
            model_bytes = fs.to_bytes(self.params)
            (path/f'{self.solver.solver_config.project_name}_{self.solver.solver_config.run_name}_model.msgpack').write_bytes(model_bytes)
        if self.solver.solver_config.save_opt:
            opt_bytes = fs.to_bytes(self.opt_state)
            (path/f'{self.solver.solver_config.project_name}_{self.solver.solver_config.run_name}_opt.msgpack').write_bytes(opt_bytes)

        self.solver.close()